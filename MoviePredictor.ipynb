{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "5850b20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "917d6c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\n\\ntry:\\n    answersDataFrane = pd.read_csv(\"AnkietaOFilmachIMDB.csv\", encoding=\\'latin1\\', sep=\\';\\')\\n    print(\"File loaded successfully with \\'latin1\\' encoding and comma delimiter.\")\\nexcept UnicodeDecodeError:\\n    try:\\n        answersDataFrane = pd.read_csv(\"AnkietaOFilmachIMDB.csv\", encoding=\\'windows-1252\\', sep=\\';\\')\\n        print(\"File loaded successfully with \\'windows-1252\\' encoding and comma delimiter.\")\\n    except Exception as e:\\n        print(f\"An error occurred with encoding or delimiter: {e}\")\\nexcept FileNotFoundError:\\n    print(\"Error: The file \\'AnkietaOFilmachIMDB.csv\\' was not found.\")\\n    '"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import pandas as pd\n",
    "\n",
    "try:\n",
    "    answersDataFrane = pd.read_csv(\"AnkietaOFilmachIMDB.csv\", encoding='latin1', sep=';')\n",
    "    print(\"File loaded successfully with 'latin1' encoding and comma delimiter.\")\n",
    "except UnicodeDecodeError:\n",
    "    try:\n",
    "        answersDataFrane = pd.read_csv(\"AnkietaOFilmachIMDB.csv\", encoding='windows-1252', sep=';')\n",
    "        print(\"File loaded successfully with 'windows-1252' encoding and comma delimiter.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred with encoding or delimiter: {e}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: The file 'AnkietaOFilmachIMDB.csv' was not found.\")\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fa1f24",
   "metadata": {},
   "source": [
    "## Data preparation for user data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "0c66d3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#answersDataFrane.replace(-1, pd.NA).describe(include='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "f78d2225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_cleaner(CurrentDataFrame, rating_name):\n",
    "\n",
    "    if CurrentDataFrame[rating_name].dtype!=\"int64\":\n",
    "\n",
    "        if CurrentDataFrame[rating_name].dtype!=\"float64\":\n",
    "            CurrentDataFrame[rating_name] = CurrentDataFrame[rating_name].astype(str).str.strip()\n",
    "            CurrentDataFrame[rating_name] = pd.to_numeric(CurrentDataFrame[rating_name], errors='coerce')\n",
    "\n",
    "        CurrentDataFrame = CurrentDataFrame.fillna(-1)    \n",
    "        if CurrentDataFrame[rating_name].dtype == 'float64':\n",
    "            CurrentDataFrame[rating_name] = CurrentDataFrame[rating_name].astype('int64')\n",
    "    return CurrentDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "46927857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- User Data preparation ---\n",
    "answersDataFrane = pd.read_csv(\"AnkietaOFilmachIMDB.csv\", encoding='latin1', sep=';')\n",
    "for col in answersDataFrane.columns[1:]:\n",
    "    answersDataFrane = rating_cleaner(answersDataFrane, col)\n",
    "\n",
    "# --- Merge movie data with user ratings using a common key ---\n",
    "# First, rename 'whose_rating' to 'name' to match the main df\n",
    "answersDataFrane.rename(columns={'whose_rating': 'name'}, inplace=True)\n",
    "# Then, merge the two DataFrames on the 'name' column\n",
    "df = pd.merge(df, answersDataFrane, on='name', how='left')\n",
    "for col in df.columns:\n",
    "    if col.startswith('rating_'):\n",
    "        df[col] = df[col].fillna(-1)\n",
    "\n",
    "# --- Data preparation for project ---\n",
    "# ... (your existing code for one-hot encoding, etc.) ...\n",
    "#df = one_hot_encode(df, (\"genre\", \"cast_name\", \"writter_name\", \"director_name\"), prefixes = (\"g_\", \"c_\", \"w_\", \"d_\") )\n",
    "\n",
    "#certificate_numbers = ([])\n",
    "#certificate_labels = df[\"certificate\"].unique()\n",
    "# ... (your certificate mapping code) ...\n",
    "#df.replace(mapping, inplace=True)\n",
    "\n",
    "#df = df.drop(columns=\"duration\")\n",
    "\n",
    "# *** REMOVE THIS LINE: df = df.dropna() ***\n",
    "\n",
    "# ... (the rest of your data preparation) ...\n",
    "# You should handle missing values within test_setups, as it's currently doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "21ce4c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "952773b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for col in answersDataFrane.columns[1:]:\\n    extracted_col = answersDataFrane[col]\\n    df = pd.concat([df, extracted_col], axis=1)'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for col in answersDataFrane.columns[1:]:\n",
    "    extracted_col = answersDataFrane[col]\n",
    "    df = pd.concat([df, extracted_col], axis=1)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31066e4f",
   "metadata": {},
   "source": [
    "## Data preparation for project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559bc580",
   "metadata": {},
   "source": [
    "### usuniecie czesci kolumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "dba917ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"img_link\" in df.columns: df.drop(columns=\"img_link\", inplace=True)\n",
    "if \"id\" in df.columns: df.drop(columns=\"id\", inplace=True)\n",
    "if \"cast_id\" in df.columns: df.drop(columns=\"cast_id\", inplace=True)\n",
    "if \"director_id\" in df.columns: df.drop(columns=\"director_id\", inplace=True)\n",
    "if \"writter_id\" in df.columns: df.drop(columns=\"writter_id\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8f96d1",
   "metadata": {},
   "source": [
    "### funkcja do zmiany stringu w listę"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "1e0a41a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_if_string(value):\n",
    "    if isinstance(value, str):\n",
    "        return value.split(',')\n",
    "    return value # Return as is if not a string (e.g., already a list or NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "a4439ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cast_name'] = df['cast_name'].apply(split_if_string)\n",
    "df['writter_name'] = df['writter_name'].apply(split_if_string)\n",
    "df['director_name'] = df['director_name'].apply(split_if_string)\n",
    "df['genre'] = df['genre'].apply(split_if_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "6d76982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['duration'] = pd.to_numeric(df['duration'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "2699eedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 60, 90, 120, 150, 180, float('inf')]\n",
    "labels = [0, 1, 2, 3, 4, 5]\n",
    "#labels = ['very short', 'short', 'normal', 'pretty long', 'long', 'very long']\n",
    "\n",
    "\n",
    "df['duration_category'] = pd.cut(df['duration'], bins=bins, labels=labels, right=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "1cfd05a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['duration_category'] = pd.to_numeric(df['duration_category'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49ad987",
   "metadata": {},
   "source": [
    "# Przygotowanie pod Sieć neuronową"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b40451",
   "metadata": {},
   "source": [
    "### usuniecie nazw, oraz rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "df3aae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if \"rank\" in df.columns: df.drop(columns=\"rank\", inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48ebb5e",
   "metadata": {},
   "source": [
    "### function for one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "aa3b98db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode (DataFrame, column_names, prefixes):\n",
    "    dfInWork = DataFrame\n",
    "    for column_name, prefix in zip(column_names, prefixes):\n",
    "        if column_name in dfInWork.columns: \n",
    "            df_exploded = dfInWork[column_name].explode()\n",
    "            df_dummies = pd.get_dummies(df_exploded, prefix=prefix)\n",
    "            df_genres_encoded = df_dummies.groupby(df_dummies.index).max()\n",
    "            df_final = pd.merge(dfInWork, df_genres_encoded, left_index=True, right_index=True, how='left')\n",
    "            dfInWork = df_final.drop(column_name, axis=1)\n",
    "        else:\n",
    "            print(f\"ERROR, column {column_name} non existing\")\n",
    "    return dfInWork\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "cafe08e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = one_hot_encode(df, (\"genre\", \"cast_name\", \"writter_name\", \"director_name\"), prefixes = (\"g_\", \"c_\", \"w_\", \"d_\") )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "48b91a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kubah\\AppData\\Local\\Temp\\ipykernel_28860\\3106441032.py:16: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(mapping, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "certificate_numbers = ([])\n",
    "certificate_labels = df[\"certificate\"].unique()\n",
    "\n",
    "i = 0\n",
    "for unique in certificate_labels:\n",
    "    certificate_numbers.append(i)\n",
    "   # certificate_labels.append(unique)\n",
    "    i+=1\n",
    "\n",
    "label_to_number_map = dict(zip(certificate_labels, certificate_numbers))\n",
    "\n",
    "mapping = {\n",
    "    \"certificate\": label_to_number_map\n",
    "}\n",
    "\n",
    "df.replace(mapping, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "7d1876bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=\"duration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81606900",
   "metadata": {},
   "source": [
    "### pozbycie się nulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "9dae4525",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b177625",
   "metadata": {},
   "source": [
    "## pozbycie się ciągłości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "5eca4176",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"imdb_rating\"] *=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e5cfaa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.replace(mapping, inplace=True)\n",
    "for i in df.columns:\n",
    "    if df[i].dtype == 'float64':\n",
    "        df[i] = df[i].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850f0a5d",
   "metadata": {},
   "source": [
    "### pozbycie się rzadkich aktorów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "286a7645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undeleted: 190\n",
      "deleted: 4277\n"
     ]
    }
   ],
   "source": [
    "deleted = 0\n",
    "undeleted = 0\n",
    "\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == \"bool\":\n",
    "        if df[column].values.sum() < 3:\n",
    "            df.drop(columns=column, inplace=True)\n",
    "            deleted += 1\n",
    "        else:\n",
    "            undeleted+=1\n",
    "print(f\"undeleted: {undeleted}\")\n",
    "print(f\"deleted: {deleted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b49ef41",
   "metadata": {},
   "source": [
    "### discretising year and number of votes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ce5ff4",
   "metadata": {},
   "source": [
    "sprawdzenie wartości dla danych lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "8594e847",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 1944, 1960, 1968, 1994, 2007, 2015, float('inf')]\n",
    "labels = [0, 1, 2, 3, 4, 5, 6]\n",
    "#labels = ['very short', 'short', 'normal', 'pretty long', 'long', 'very long']\n",
    "\n",
    "\n",
    "df['year_category'] = pd.cut(df['year'], bins=bins, labels=labels, right=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "bb60f812",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=\"year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ca0c2b",
   "metadata": {},
   "source": [
    "### to samo, ale z liczbą głosów"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e61ebb7",
   "metadata": {},
   "source": [
    "od 1740000 są wysokie,\n",
    "from 1030000 moderate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "3a258ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_ranges = [0, 2*10**5, 4*10**5, 7*10**5, 10**6, float('inf')]\n",
    "labels = [0, 1, 2, 3, 4, 5, 6]\n",
    "#labels = ['very short', 'short', 'normal', 'pretty long', 'long', 'very long']\n",
    "\n",
    "\n",
    "df['imbd_votes_category'] = pd.cut(df['imbd_votes'], bins=bins, labels=labels, right=False)\n",
    "df = df.drop(columns=\"imbd_votes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4d54a3",
   "metadata": {},
   "source": [
    "# Split The DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "15d79015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Modelling\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Tree Visualisation\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fb94af",
   "metadata": {},
   "source": [
    "Choice of surveyed person"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727dfe6c",
   "metadata": {},
   "source": [
    "# Train & test the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "60aa9a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "66b9d68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "0bdcafb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_human_ai_difference(xs, ys):\n",
    "    AvgDifference = 0\n",
    "    for (x,y) in zip(xs, ys):\n",
    "        AvgDifference += abs(x-y)**3\n",
    "    AvgDifference /= len(xs)\n",
    "    return AvgDifference\n",
    "\n",
    "def test_setups(dataFrame, numberOfIterations, probeSize, predictColumns, neuralNetworkParametersSets, getPredictions):\n",
    "    savedTestPredSet = ([])\n",
    "    globalParameterAverageDifferenceMeanArray = ([])\n",
    "    globalColumnAverageDifferenceMeanArray = ([])\n",
    "    \n",
    "# - - - - - - - - - - - - Calculating for every paramether set - - - - - - - - - - - - \n",
    "    for neuralNetworkParametersSet in neuralNetworkParametersSets:\n",
    "        parameterAverageDifferenceMeanArray = ([])\n",
    "        # Get the data (ready for sgd and adam)\n",
    "     #   activation, solver, alpha, learning_rate_init, hidden_layer_sizes, max_iter = neuralNetworkParametersSet\n",
    "        \n",
    "        #Mean of average difference for one parameter set\n",
    "        parametersAverageDifferenceMean = 0\n",
    "\n",
    "        # Mean of average differences for every one column saved\n",
    "        columnAverageDifferenceMeanArray = ([])\n",
    "       # temporaryDataFrame = uneditedDataFrame.copy()\n",
    "    # - - - - - - - - - - - - Calculating for every user input - - - - - - - - - - - - \n",
    "        for predictColumn in predictColumns:\n",
    "        #    temporaryDataFrame = uneditedDataFrame.copy()\n",
    "            # Mean of average differences for one column\n",
    "            columnAverageDifferenceMean = 0   \n",
    "        # - - - - - - - - - - - - Calculating for every iteration - - - - - - - - - - - - \n",
    "            for iteration in range(0, numberOfIterations, 1):\n",
    "                temporaryDataFrame = dataFrame.copy()\n",
    "\n",
    "\n",
    "                # Drop every column with \"rating_\" that isnt the one to be predicted\n",
    "                for col in list(temporaryDataFrame.columns):\n",
    "                    if col.startswith(\"rating_\") and col != predictColumn:\n",
    "                        # Added: Check if column still exists before attempting to drop\n",
    "                        if col in temporaryDataFrame.columns:\n",
    "                            temporaryDataFrame.drop(columns=col, inplace=True)\n",
    "\n",
    "\n",
    "                # Getting rid of unfilled data (to be used in the future)\n",
    "                training_data = temporaryDataFrame[temporaryDataFrame[predictColumn] != -1]\n",
    "                prediction_data = temporaryDataFrame[temporaryDataFrame[predictColumn] == -1]\n",
    "\n",
    "                \n",
    "                if temporaryDataFrame.empty:\n",
    "                    print(f\"Warning: temporaryDataFrame became empty after filtering for '{predictColumn}'. Skipping iteration.\")\n",
    "                    continue # Skip this iteration if no data remains\n",
    "\n",
    "\n",
    "                # Splitting the dataset into train and test\n",
    "\n",
    "\n",
    "                \n",
    "              #  X = temporaryDataFrame.drop([predictColumn, 'name'], axis=1)\n",
    "                #X_names = temporaryDataFrame['name']\n",
    "               # y = temporaryDataFrame[predictColumn]\n",
    "               # XTrain, XTest, yTrain, yTest = train_test_split(X, y, test_size=probeSize, random_state=42)\n",
    "\n",
    "                XTrainNames = training_data['name']\n",
    "                XTrain = training_data.drop(columns=[predictColumn, 'name'])\n",
    "                yTrain = training_data[predictColumn]\n",
    "\n",
    "               # XTest = toBePredicted.drop(predictColumn, axis=1)\n",
    "                XTest = prediction_data.drop(columns=[predictColumn, 'name'])\n",
    "                XTest_names = prediction_data['name']\n",
    "\n",
    "                # Training the network\n",
    "                clf = MLPRegressor(**neuralNetworkParametersSet)\n",
    "              #  clf = MLPClassifier(activation = activation, solver=solver, alpha=alpha,\n",
    "               #                     learning_rate_init = learning_rate_init,\n",
    "                #                    hidden_layer_sizes=hidden_layer_sizes, max_iter = max_iter)\n",
    "                \n",
    "                x_scaler = StandardScaler()\n",
    "                y_scaler = StandardScaler()\n",
    "                X_train_scaled = x_scaler.fit_transform(XTrain)\n",
    "                X_test_scaled = x_scaler.transform(XTest)\n",
    "                y_train_scaled = y_scaler.fit_transform(yTrain.values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "                clf.fit(X_train_scaled, y_train_scaled.ravel())\n",
    "\n",
    "                # Testing the network\n",
    "                y_pred_scaled = clf.predict(X_test_scaled)\n",
    "                yPred = y_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1))\n",
    "\n",
    "                #columnAverageDifferenceMean += calculate_average_human_ai_difference(yTest, yPred)\n",
    "                if getPredictions:\n",
    "                    print(f\"DEBUG: Length of XTest_names: {len(XTest_names)}\")\n",
    "                    print(f\"DEBUG: Length of yPred: {len(yPred)}\")\n",
    "                    # Store the input features (XTest) and the predictions (yPred)\n",
    "                    savedTestPredSet = (XTest_names, yPred)\n",
    "                    # This print is now for debugging and can be removed later\n",
    "                    # print(yPred) dSet = (yTest, yPred)\n",
    "            # Mean of average difference in current column prediction\n",
    "            columnAverageDifferenceMean /= numberOfIterations\n",
    "            # Saving such mean\n",
    "            columnAverageDifferenceMeanArray.append(columnAverageDifferenceMean)\n",
    "\n",
    "        # Mean of average difference in current parameter set\n",
    "        parametersAverageDifferenceMean = np.mean(columnAverageDifferenceMeanArray)\n",
    "        parameterAverageDifferenceMeanArray.append(parametersAverageDifferenceMean)\n",
    "        globalColumnAverageDifferenceMeanArray.append(columnAverageDifferenceMeanArray)\n",
    "\n",
    "    #globalParameterAverageDifferenceMeanArray.append(parametersAverageDifferenceMean)\n",
    "       # print(\"Appending\")\n",
    "        globalParameterAverageDifferenceMeanArray.append(parameterAverageDifferenceMeanArray) # Use extend to flatten if parameterAverageDifferenceMeanArray is a list of single values\n",
    "    return globalParameterAverageDifferenceMeanArray, globalColumnAverageDifferenceMeanArray, savedTestPredSet\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "1249276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Ignore all ConvergenceWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "5ff8aebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Length of XTest_names: 201\n",
      "DEBUG: Length of yPred: 201\n",
      "Predictions saved to predicted_ratings('rating_kuby',).csv\n",
      "                    name  predicted_('rating_kuby',)\n",
      "0           12 Angry Men                    6.265747\n",
      "1       12 Years a Slave                    7.560366\n",
      "2                   1917                    6.751972\n",
      "3  2001: A Space Odyssey                    8.524257\n",
      "4               3 Idiots                    8.043756\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "predictColumns = (\"rating_kuby\",)\n",
    "\n",
    "\n",
    "activationSet = (\"relu\",)\n",
    "solverSet = (\"adam\",)\n",
    "alphaSet = (0.1, )\n",
    "learning_rate = (\"adaptive\",)\n",
    "learning_rate_initSet =  (0.1,)\n",
    "hidden_layer_sizesSet = ((8,16, 32, 16, 8),)\n",
    "max_iterSet = (100000, )\n",
    "\n",
    "# Generate all combinations\n",
    "neuralNetworkParametersSets = []\n",
    "for activation, solver, alpha, learning_rate, learning_rate_init, hidden_layer_sizes, max_iter in itertools.product(\n",
    "    activationSet,\n",
    "    solverSet,\n",
    "    alphaSet,\n",
    "    learning_rate,\n",
    "    learning_rate_initSet,\n",
    "    hidden_layer_sizesSet,\n",
    "    max_iterSet\n",
    "):\n",
    "    param_set = {\n",
    "        \"activation\": activation,\n",
    "        \"solver\": solver,\n",
    "        \"alpha\": alpha,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"learning_rate_init\": learning_rate_init,\n",
    "        \"hidden_layer_sizes\": hidden_layer_sizes,\n",
    "        \"max_iter\": max_iter\n",
    "    }\n",
    "    neuralNetworkParametersSets.append(param_set)\n",
    "\n",
    "#print(f\"DEBUG: Number of neuralNetworkParametersSets generated: {len(neuralNetworkParametersSets)}\")\n",
    "#print(f\"DEBUG: Contents of neuralNetworkParametersSets: {neuralNetworkParametersSets}\")\n",
    "\n",
    "parameter, _, savedData = test_setups(dataFrame = df, numberOfIterations = 1,\n",
    "                                              probeSize = 0.3, predictColumns=predictColumns,neuralNetworkParametersSets=neuralNetworkParametersSets,\n",
    "                                              getPredictions=True)\n",
    "\n",
    "chosenRating = predictColumns\n",
    "if savedData:\n",
    "    XTest_names_output, yPred_output = savedData\n",
    "    \n",
    "    yPred_output = yPred_output.flatten()\n",
    "    \n",
    "    # Create a new DataFrame with the names and predictions\n",
    "    results_df = pd.DataFrame({\n",
    "        \"name\": XTest_names_output.reset_index(drop=True),\n",
    "        f\"predicted_{predictColumns}\": yPred_output\n",
    "    })\n",
    "    \n",
    "    # --- Save the DataFrame to a new CSV file ---\n",
    "    results_df.to_csv(f'predipocted_ratings{predictColumns}.csv', index=False)\n",
    "    \n",
    "    print(f\"Predictions saved to predicted_ratings{predictColumns}.csv\")\n",
    "    print(results_df.head())\n",
    "else:\n",
    "    print(\"No predictions were saved.\")\n",
    "#print(parameter)\n",
    "#print(globalTrainingSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "5f4907ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "3ee984bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[215], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[0;32m      8\u001b[0m chosenRating \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrating_ali\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m yTest, yPred \u001b[38;5;241m=\u001b[39m globalTrainingSet\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# --- DEBUGGING AND FIX START ---\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of yTest before flattening: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myTest\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from collections import Counter # Import Counter for easily counting occurrences\n",
    "from matplotlib.colors import Normalize # For normalizing counts to colormap range\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "chosenRating = \"rating_ali\"\n",
    "yTest, yPred = globalTrainingSet\n",
    "\n",
    "# --- DEBUGGING AND FIX START ---\n",
    "print(f\"Shape of yTest before flattening: {yTest.shape}\")\n",
    "print(f\"Shape of yPred before flattening: {yPred.shape}\")\n",
    "\n",
    "# Flatten the arrays to ensure they are 1D\n",
    "yTest_flat = np.ravel(yTest)\n",
    "yPred_flat = np.ravel(yPred)\n",
    "\n",
    "# --- DEBUGGING AND FIX END ---\n",
    "\n",
    "minimal = yTest_flat.min() if yTest_flat.min() < yPred_flat.min() else yPred_flat.min()\n",
    "maximal = yTest_flat.max() if yTest_flat.max() > yPred_flat.max() else yPred_flat.max()\n",
    "\n",
    "x = range(math.floor(minimal),math.ceil(maximal+1),1)\n",
    "y = range(math.floor(minimal),math.ceil(maximal+1),1)\n",
    "\n",
    "# Pass the flattened arrays directly to zip\n",
    "data_points = list(zip(yTest_flat, yPred_flat))\n",
    "\n",
    "point_counts = Counter(data_points)\n",
    "\n",
    "unique_points = list(point_counts.keys())\n",
    "counts = np.array(list(point_counts.values()))\n",
    "\n",
    "min_count = counts.min()\n",
    "max_count = counts.max()\n",
    "\n",
    "#('viridis', 'plasma', 'hot', 'magma', 'cool')\n",
    "cmap = cm.plasma\n",
    "\n",
    "norm = Normalize(vmin=min_count, vmax=max_count if max_count > min_count else min_count + 1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for (test_val, pred_val) in unique_points:\n",
    "    count = point_counts[(test_val, pred_val)]\n",
    "    color = cmap(norm(count))\n",
    "    plt.scatter(test_val, pred_val, color=color, s=50, alpha=0.7)\n",
    "\n",
    "plt.plot(x, y, color='gray', alpha=0.6, linestyle='--', label='Idealne przewidywanie')\n",
    "\n",
    "plt.xlabel(\"Odpowiedź człowieka\")\n",
    "plt.ylabel(\"Przewidziana odpowiedź przez algorytm\")\n",
    "\n",
    "sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm, ax=plt.gca())\n",
    "cbar.set_label('Liczba punktów o tej samej pozycji')\n",
    "\n",
    "\n",
    "chosenRating_name = chosenRating.replace(\"rating_\", \"\")\n",
    "plt.title(f\"Przewidywanie oceny ankietowanego ( {chosenRating_name} ) wybranych filmów z top250 filmów IMDb\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=':', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a97a1d6",
   "metadata": {},
   "source": [
    "# wybrany setup:\n",
    "activationSet = (\"relu\",)\n",
    "\n",
    "solverSet = (\"adam\",)\n",
    "\n",
    "alphaSet = (0.1, )\n",
    "\n",
    "learning_rate = (\"adaptive\",)\n",
    "\n",
    "learning_rate_initSet =  (0.1,)\n",
    "\n",
    "hidden_layer_sizesSet = ((8,16, 32, 16, 8),)\n",
    "\n",
    "max_iterSet = (100000, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b070e322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "predictColumns = (\"rating_kuby\", \"rating_ali\")\n",
    "\n",
    "\n",
    "activationSet = (\"relu\",)\n",
    "solverSet = (\"adam\",)\n",
    "alphaSet = (0.1, )\n",
    "learning_rate = (\"adaptive\",)\n",
    "learning_rate_initSet =  (0.1,)\n",
    "hidden_layer_sizesSet = ((8,16, 32, 16, 8),)\n",
    "max_iterSet = (100000, )\n",
    "\n",
    "# Generate all combinations\n",
    "neuralNetworkParametersSets = []\n",
    "for activation, solver, alpha, learning_rate, learning_rate_init, hidden_layer_sizes, max_iter in itertools.product(\n",
    "    activationSet,\n",
    "    solverSet,\n",
    "    alphaSet,\n",
    "    learning_rate,\n",
    "    learning_rate_initSet,\n",
    "    hidden_layer_sizesSet,\n",
    "    max_iterSet\n",
    "):\n",
    "    param_set = {\n",
    "        \"activation\": activation,\n",
    "        \"solver\": solver,\n",
    "        \"alpha\": alpha,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"learning_rate_init\": learning_rate_init,\n",
    "        \"hidden_layer_sizes\": hidden_layer_sizes,\n",
    "        \"max_iter\": max_iter\n",
    "    }\n",
    "    neuralNetworkParametersSets.append(param_set)\n",
    "\n",
    "print(f\"DEBUG: Number of neuralNetworkParametersSets generated: {len(neuralNetworkParametersSets)}\")\n",
    "print(f\"DEBUG: Contents of neuralNetworkParametersSets: {neuralNetworkParametersSets}\")\n",
    "\n",
    "parameter, _, globalTrainingSet = test_setups(dataFrame = df, numberOfIterations = 1,\n",
    "                                              probeSize = 0.3, predictColumns=predictColumns,neuralNetworkParametersSets=neuralNetworkParametersSets,\n",
    "                                              getPredictions=True)\n",
    "print(parameter)\n",
    "print(globalTrainingSet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
